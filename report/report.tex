% https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{parselines}
\usepackage{listings}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{A System Analysis to Dkron Scheduler}


%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Miguel Albuquerque, 1105828\inst{1}\orcidID{1105828}}
\institute{Instituto Superior TÃ©cnico, Av. Rovisco Pais 1, 1049-001 Lisboa, Portugal
\email{miguel.albuquerque@tecnico.ulisboa.pt}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
WRITE ABSTRACT HERE
\end{abstract}

\par
\text{Repository: \href{https://github.com/miguel-msa/dkron-study}{github.com/miguel-msa/dkron-study}}
\par
\text{Commit hash:}\textbf{PUT COMMIT HASH HERE}

\keywords{Dkron \and Distributed \and Benchmark \and Scalability \and Consensus \and Dkron Server \ and Dkron Agent \and Raft.}
%
%
%
% %%%
% %%%
\begin{figure}
\centering
\includegraphics[width=0.25\textwidth]{media/dkron-logo.png}
%\caption{dkron Logo .} \label{fig1}
\end{figure}
\section{Introduction}
TODO: add
- context on what are these systems
- problem it solves
- contextualize dkron into the above
%brief presentation of the selected system and the justification for its choice.
%The report should also specify the git commit identifier, and the url and hash of the video that should be considered for evaluation purposes. Failure to do so will result in a grade penalty.

On the surface, Dkron provides an interface that:
\begin{enumerate}
    \item Has flexible tag-based job definition
    \item Allows for Distributed execution, where these jobs can be ran in a distributed mode. % todo: review this!
    \item Has no Single Point of Failure.
    \item Easy to deploy with built-in replicated storage (BuntDB) relying on the Raft protocol.
    \item Provides a Web-GUI for administration.
\end{enumerate}
The above characteristics, and more, are further explored in \ref{system_description}

\break
When selecting the system to analyse, some of the factors driving the ultimate choice were:
\begin{enumerate}
    \item Interest in exploring \href{https://go.dev/}{Go} source code.
    \item Understand more about a system that is not a database
    \item IS THIS OK? REVIEW THIS
\end{enumerate}
After finding some systems fulfilling these requirements, Dkron became the system that raised a curious question:
\textit{What is the performance of a scheduler, seemingly, focused on availability? and reliability?}

As this is not explored by Dkron, I became interested in finding out the implementation, e.g. scheduling approach, and extrapolate,
joined with benchmarking, its behavior, i.e., its performance, and scaling capacity.

~\\
Ultimately, Dkron is a golang written scheduler with specific particularities for some key use-cases.
Its claims are not bold, neither on its performance, nor on how it actually does scheduling. This opens an
opportunity to not only explore something unclaimed, but also on how Dkron found the
optimal point, if exists, between performance and reliability.

%Some of the claimed use-cases are:
%- Email delivery
%- Payroll generation
%- Bookkeeping
%- Data consolidation for BI
%- Recurring invoicing
%- Data transfer

% TODO
% ! TODO: this text needs to change
% TODO: these claims are NOT CAREFULLY CONSIDERED! REVIEW THIS!!!
~\\ The analysis will consider these use-cases and apply a ...benchmark... to analyse the performance and infer Dkron's
performance on workloads similar, as in trying to represent, to such use-cases.
% TODO: these claims are NOT CAREFULLY CONSIDERED! REVIEW THIS!!!

\section{System Description}
\label{system_description}

PUT HERE ALL THE relevant PERKS\&QUIRKS of Dkron in detail

Always available: Using the power of the Raft protocol, Dkron is designed to be always available. If the cluster leader node fails, a follower will replace it, all without human intervention.

Flexible targets: Simple but powerful tag-based target node selection for jobs. Tag node count allows to run jobs in an arbitrary number of nodes in the same group or groups.

% todo: summarize the dkron relevant doc. sections

\subsection{Getting started}
Dkron has 2 kinds of nodes:
\begin{itemize}
    \item Agent
    \item Server
\end{itemize}
Both are cluster members available to run scheduled jobs
An agent can handle job executions, run scripts and return the resulting output to the server.
A server does everything an Agent does (does it? CHECK better), plus:
\begin{itemize}
    \item schedules the jobs
    \item handles data storage
    \item participate on leader (Raft) election
    \item dispatches the execution of jobs either to Agents, or other Servers (CHECK: only other servers???).
\end{itemize}

A Dkron cluster has a leader, for Raft based distribution, which is responsible to start job execution queries in the cluster.

By default, all nodes execute each job. This can be controlled through the use of tags and
a count of target nodes having this tag. "This allows to run jobs across a cluster of any size and with any
combination of machines needed."

All execution responses are gathered by the scheduler and stored in the BuntDB storage.

--- SHOW HERE A CODE EXAMPLE OF TAG-BASED JOB SCHEDULE ---

State Storage: BuntDB
The scheduler state is replicated (not throughout, its different, CHECK) between all server nodes using the Serf.

--
Dkron can either run on a single node (by default) or on a cluster with multiple nodes.

CHECK: Create a Job @ https://dkron.io/docs/basics/getting-started more details there might be interesting

\subsection{Intro}
THIS INFO MAY BE TRANSFERRED TO INTRO AND INTRO MAY SUFFER CHANGES!!! REVIEW

\subsection{Relevant/Interesting characteristics}
\begin{enumerate}
    \item \textbf{Concurrency:} A job can be either ran concurrently, by default,
    allowing for overlapping jobs, or forbid concurrency.
    \item \textbf{Cron spec support:} A job can be defined either by using Dkron's
    pre-defined schedules, which are more.
    declarative, e.g., \textit{@hourly}, or using cron expression format.
    \item \textbf{Executors:} Plugins to execute the jobs, e.g., Shell to run the job in
    the current node, or HTTP to send a request.
    \item \textbf{Metrics:} Dkron can send metrics to Statsd and provide prometheus format metrics.
    \item \textbf{Job Retries:} A job can be declared to retry in case of failure.
    \item \textbf{Embedded Storage:} Dkron uses an embedded KV store based on BuntDB.
    \item \textbf{Target nodes spec:} By default every node will run a job, however there is
    the ability to set tags for jobs to be ran only by target nodes identified with those
    same tags.
    \item \textbf{Clustering:} Clustering in Dkron provides fault tolerance when the
    quorum is equal or higher than 3.
\end{enumerate}

% todo: make this a paragraph instead ???
\subsection{No Single Point of Failure}
This no SPOF characteristic is very relevant for systems that depend on workload automation to function i.e. the scheduler dependee
system might be fault-tolerant, but the scheduler itself might not, making these "fault-tolerant" systems, indirectly, not as
so - Dkron fixes this problem by, as they claim, being the only existing scheduler with no SPOF. % ! REPETITIVE TEXT

% ? Acceptable Performance --> we'll find out on tests
Therefore, with acceptable performance, whilst providing Reliability? and Availability?, Dkron is an interesting solution
for use-cases that must guarantee fault-tolerance.

\section{Benchmarking}

Due to the project time constraints, an extensive literature review was not performed, however
a great portion of this project's approach, execution and reporting is done based on the acquired
knowledge in the course.

During the limited research, I could not find a convincing standard benchmark to use or be based on.
Instead, I focused on researching the particularities of Dkron and exploiting those to gather conclusions and,
admittedly, confirm some deductions I was forming throughout Dkron's design analysis.

One of the clearest characteristic for performance analysis in Dkron is its use of Raft to guarantee
reliability. This, along the fact that all the dkron cluster members of kind Server persist,
between themselves, all the job responses' logs.

~\\ Thus, the intuitive conclusion for Dkron is:
\begin{itemize}
    \item Increase Dkron Servers to improve fault-tolerance, but increases overhead.
    \item Increase Dkron Agents to improve performance.
\end{itemize}


Nevertheless, solely increasing Agents, even with unlimited costs, is not an infinite answer.
Even more so considering that, unless the
\href{job is targeted to a set of tags}{https://dkron.io/docs/usage/target-nodes-spec},
every member in the cluster will run the job.

With very limited, information on Dkron's scaling capabilities either on their
documentation or elsewhere, one can assume, contention and crosstalk will increase as most systems do.
Without a formal baseline to which to compare with, I took the liberty to thinker with a synthetic workload
to push the limits of Dkron.

Having this, considering the Universal Scalability Law, the properties that would most
affect Dkron scalability could be:

\begin{itemize}
    \item Increase in Dkron Servers may increase:
     \begin{itemize}
        \item Contention: since there are serial parts of the dkron process like,
        receiving, dispatching, and storing a job. The serial part of the job itself, will
        depend on the process to execute associated to the job, this can be anything.
        \item Crosstalk: this can be assumed to be a big part of these kinds of members since
        a server needs frequent communication between its consensus peers, e.g., for
        leader election, and state coherence.
     \end{itemize}
    \item Increase in Dkron Agents may increase:
     \item Contention: having more members in the cluster increases the cluster provisioning, maintenance,
     and load balancing overhead.
     \item Crosstalk: although not as much as Dkron Servers, but Agents need to join the cluster and become
     known by the Servers for job dispatch, as well every new agent is a new running job that needs (by default)
     to be stored by servers, requiring state coherence.
\end{itemize}

% The stated above needs to be empirically tested, as there is not much information on the
% internal qualities of Dkron. The analysis on this project are mostly heuristic based on the
% theoretical lectures of the course, and plays a role not only on performance analysis, but also
% on exploring the system itself and the ways of its deployment.


\subsubsection{Built Heuristic Benchmak}

The benchmark was built with \href{K6}{https://k6.io/docs/} in JavaScript.

I did experiment with following:
\begin{itemize}
    \item CPU-focused job: doing a high bound loop with arithmetic computations
    \item Memory-focused job: read data and pipe it
\end{itemize}

However, these jobs proved to be very heavy for the Dkron pods, as it would cause crashes and even no
fault recovery unless manually fixed.

The final benchmark does 2 simple jobs that do the same but in different manners:
\begin{itemize}
    \item schedule echo - this job will run indefinitely on the frequency defined
    \item immediately request echo
\end{itemize}

These jobs are either ran by the pod that received the job, or dispatched, by a Server, to another pod.

Regarding immediately requesting echo, k6 provides an interface to simulate multiple virtual users
making requests.

% \begin{lstlisting}[language=javascript]
% export const options = {
%     stages: [
%         { duration: "60s", target: 20 },
%         { duration: "60s", target: 60 },
%         { duration: "60s", target: 0 },
%     ],
% };
% \end{lstlisting}
\begin{verbatim}
export const options = {
    stages: [
        { duration: "60s", target: 20 },
        { duration: "60s", target: 60 },
        { duration: "60s", target: 0 },
    ]
};
\end{verbatim}

As observed in the code in FIGX, this benchmarks runs for 3 minutes ramping up virtual
users to 20 during the first sixty seconds, then ramping up from 20 to 60 in the next sixty seconds,
and then ramping down.

For more detail on the benchmark refer to \ref{}, where it is explained in
more detail after discussing the Experimental Design.

% ! TODO: make this after explaining FACTORS ???
The script can be analysed in \textit{/benchmark/benchmark.js}. The general idea is:
\begin{itemize}
    \item When the fac
\end{itemize}



--
Performance Metrics:
- talk about what are they and which I selected
- brief on latency vs throughput
- talk about performance limitations and which I had
--

Due to lack of material and examples on benchmarking job schedulers, the benchmarking was heuristics.

\subsection{Dkron Under the Universal Scalability Law}

1. Devise a simple benchmark that allows to assess the behaviour of the system.
2. Analyse the scalability properties of the system.
3. Analyse how the scalability properties can be explained considering the Universal Scalability Law.

\subsection{Dkron Bottlenecks}
1. Decompose a request into a pipeline of stages.
2. Study which portions of the pipeline are the bottleneck (or can become one) and reason
about which of the techniques studied in the classes could be applied to remove or
mitigate the bottleneck.


\subsection{Experimental Design}
DESCRIPTION AND JUSTIFICATION OF THE EXPERIMENTAL DESIGN

After analysing Dkron's design and particularities, the Experimental Design was initiated with the
goal to extrapolate the most possible information out of the decisions that will be discussed below.

A Fractional Factorial Design was used in order to use more factors (minimum of 6), whilst reducing
the amount of experiments required to be done.

A $2^{k-p}$ was used with \textit{k=6} and \textit{p=3} combined factors, resulting
in a total of 8 experiments. Below is the signed table FIGX and respective resulting effects FIXG.

\begin{figure}
\centering
\includegraphics[width=0.25\textwidth]{media/dkron-logo.png}
%\caption{dkron Logo .} \label{fig1}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.25\textwidth]{media/dkron-logo.png}
%\caption{dkron Logo .} \label{fig1}
\end{figure}



\subsubsection{Factors}
As can be observed, the factors are the following:
\begin{itemize}
    \item HasOnDemandJobs
    \item CPU Limit (m)
    \item Memory Limit (GiB)
    \item Dkron Servers
    \item Dkron Agents
    \item Concurrent
\end{itemize}

Regarding the rationale for the choice of each factor. When deciding for factors,
after analysing the particularities of Dkron, some questions were raised:
\begin{enumerate}
    \item question1
    \item question2
\end{enumerate}

The conclusions from the analysis and consequently, the attempt to answer the above questions
can be found in subsection \ref{results}

\par \textbf{HasOnDemandJobs:} Dkron is mainly built to schedule recurring jobs,
and [this github](https://github.com/distribworks/dkron/issues/578) issue confirms that it was not initially planned to support immediately,
one time, executed jobs. The main exploratory idea was to analyse how immediate jobs are reacted to
i.e. if they have some threshold to be accepted in case all the pods are busy, or if in the case
all the pods are busy, the request is simply rejected.
\par \textbf{CPU Limit(m):} Dkron uses Raft to achieve consensus between the Dkron Agents.
Distributed consensus protocols usually are expected to be CPU and Memory bound therefore,
this factor is relevant to observe the impact of the Dkron Servers realizing consensus and
providing reliability.
\par \textbf{Memory Limit(GiB):} Likewise, due to the actions of state management and replication
being heavily required and frequent in these kinds of protocols, Memory also seemed a relevant
factor to take into account.
\par \textbf{Dkron Agents:} A Dkron Agent has a very simple goal, to execute jobs dispatched by
Dkron Servers. Having this simple pod that does not participate on consensus and only executes jobs
seemed to be the clear factor to increase for performance gains.
\par \textbf{Dkron Servers:} Dkron Servers do what Dkron Agents do plus scheduling of jobs,
handling data storage and participating on leader election. Since these 2 present very
distinguishable characteristics, it seemed relevant to analyse their interaction.
\par \textbf{Concurrent:} Dkron is states that if no Dkron Agent/Server is available

Dkron Agents in particular was a factor that unfortunately could not be configured in the
desired way to extrapolate clear conclusions and answer the initial questions.


MEMORY LIMIT NOTES: although no memory heavy jobs are done, memory usage was very high (raft?)



\subsubsection{Factor Levels}
Acknowledging there is the method of $2^k$ Factorial Design, it would be interesting to
try with some minimum and maximum values to look for \textit{unidirectional effects} and
in case the factor is has a unidirectional effect decide on the values from there. However,
from deploying an Azure Kubernetes Service to Dkron becoming ready was taking approximately
10 minutes and doing a Rolling Update was proving to be challenging, from the non-clear dkron's
helm chart to the particular mechanism of Dkron's clustering (\ref{clustering}).

Considering those challenges and the limited time, the approach to find the levels was heuristics:
\begin{itemize}
    \item For computational resources, i.e., CPU Limit and Memory Limit, an optimal point between
    these resources and prices was the consideration.
    \item For the ratio between Dkron Agents and Dkron Servers, an initial goal was set, until
    challenges were found, making it unable to use the desired levels, servers (3 to 5) and agents (0 to 3).
    Below an oversimplification of the challenge will be shared.
    \item For the benchmark workload parameters, these are simply binary values.
\end{itemize}

As for the challenge with setting the desired Dkron Agents replicas, it seems dkron helm chart
is still neither stable, nor prioritized by Dkron's team, an unfortunate late discovery
as it was unfeasible to leave the already too invested kubernetes and helm deployment leaving.
the only option to analyse with Dkron Agent as more of a binary value, 0 or 1.
Particularly, the issues seems to be with one of the templates \textit{agent-deployment.yaml} or
\textit{agent-hpa} which is ignoring the agent's set replica count and only creates 1,
independently of the set value, 0 or 100.



\subsubsection{Confounded Factors}
Confounding is used to reduce the number of ran experiments while being able to use a broader range
of factors. It is relevant to note that reducing the number of experiments in favor of time and cost savings
and sole increase in factors is not the best approach. Instead, it should be given preference to confound
factors while maintaining a higher level of experiments where their experiment error can be considered
in the effect results.

When confounding factors we should confound 1) factors where the interaction is probable to be negligible,
2) factors where combined effects are not so relevant to the analyis.

\begin{itemize}
    \item \textbf{HasOnDemandJobs + CPU Limit for Dkron Servers:} these 2 confounded factors are unlikely to interact since the expected jobs done by
        a scheduler are not heavy, and sometimes of workflow continuation or triggering.
        Both of these confounded factors would interact more if confounded with, e.g., Dkron Servers,
        for reasons of consensus overhead already explored.
    \item \textbf{HasOnDemandJobs and Memory confounded for Dkron Agents:} same rationale as
    HasOnDemandJobs + CPU Limit.
    \item \textbf{CPU Limit and Memory Limit for Concurrency}: same rationale as HasOnDemandJobs
    + CPU Limit.
\end{itemize}



\section{Results}
\label{results}
\section{Conclusion}
\label{conclusion}

\subsection{Limitations}
Dkron's helm chart is still not stable for an easy deployment. Documentation is lacking, and
it seems there is no stable helm chart for dkron. Additionally, during bug fixing and deployment
problem resolving, I was left with the idea that [a lot more other developers face their own
unexpected issues with dkron's helm chart](https://github.com/distribworks/dkron/issues?q=helm).


% ! %%%
% ! %%% EXAMPLES BELOW
% ! %%%

\begin{table}
\caption{Table captions should be placed above the
tables.}\label{tab1}
\begin{tabular}{|l|l|l|}
\hline
Heading level &  Example & Font size and style\\
\hline
Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
\hline
\end{tabular}
\end{table}


\noindent Displayed equations are centered and set on a separate
line.
\begin{equation}
x + y = z
\end{equation}
Please try to avoid rasterized images for line-art diagrams and
schemas. Whenever possible, use vector graphics instead (see
%Fig.~\ref{fig1}).

\begin{figure}
\includegraphics[width=\textwidth]{fig1.eps}
\caption{A figure caption is always placed below the illustration.
Please note that short captions are centered, while long ones are
justified by the macro package automatically.} \label{fig1}
\end{figure}

\begin{theorem}
This is a sample theorem. The run-in heading is set in bold, while
the following text appears in italics. Definitions, lemmas,
propositions, and corollaries are styled the same way.
\end{theorem}
%
% the environments 'definition', 'lemma', 'proposition', 'corollary',
% 'remark', and 'example' are defined in the LLNCS documentclass as well.
%
\begin{proof}
Proofs, examples, and remarks have the initial word in italics,
while the following text appears in normal font.
\end{proof}
For citations of references, we prefer the use of square brackets
and consecutive numbers. Citations using labels or the author/year
convention are also acceptable. The following bibliography provides
a sample reference list with entries for journal
articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
and a homepage~\cite{ref_url1}. Multiple citations are grouped
\cite{ref_article1,ref_lncs1,ref_book1},
\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.

\begin{credits}
\subsubsection{\ackname} A bold run-in heading in small font size at the end of the paper is
used for general acknowledgments, for example: This study was funded
by X (grant number Y).

\subsubsection{\discintname}
It is now necessary to declare any competing interests or to specifically
state that the authors have no competing interests. Please place the
statement with a bold run-in heading in small font size beneath the
(optional) acknowledgments\footnote{If EquinOCS, our proceedings submission
system, is used, then the disclaimer can be provided directly in the system.},
for example: The authors have no competing interests to declare that are
relevant to the content of this article. Or: Author A has received research
grants from Company W. Author B has received a speaker honorarium from
Company X and owns stock in Company Y. Author C is a member of committee Z.
\end{credits}
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\begin{thebibliography}{8}
\bibitem{ref_article1}
Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

\bibitem{ref_lncs1}
Author, F., Author, S.: Title of a proceedings paper. In: Editor,
F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
Springer, Heidelberg (2016). \doi{10.10007/1234567890}

\bibitem{ref_book1}
Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
Location (1999)

\bibitem{ref_proc1}
Author, A.-B.: Contribution title. In: 9th International Proceedings
on Proceedings, pp. 1--2. Publisher, Location (2010)

\bibitem{ref_url1}
LNCS Homepage, \url{http://www.springer.com/lncs}, last accessed 2023/10/25
\end{thebibliography}
\end{document}
